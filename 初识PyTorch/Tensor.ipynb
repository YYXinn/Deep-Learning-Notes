{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b648c28",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "- PyTorch把对数据的存储和操作都封装在Tensor里\n",
    "- GPU计算加速\n",
    "- 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b75c2d",
   "metadata": {},
   "source": [
    "### Tensor是多维数组：\n",
    "- 标量（0维）：单个数，比如 torch.tensor(3.14)\n",
    "- 向量（1维）：一列数，比如 torch.tensor([1,2,3])\n",
    "- 矩阵（2维）：行列数据，比如 torch.tensor([[1,2],[3,4]])\n",
    "- 高维张量（3维及以上）：高维数据，比如torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfdf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.int64\n",
      "cpu\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"创建一个Tensor\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "# 1D Tensor\n",
    "t1 = torch.tensor([1,2,3])\n",
    "print(t1)\n",
    "# 2D Tensor\n",
    "t2 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(t2)\n",
    "# 3D Tensor\n",
    "t3 = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])\n",
    "print(t3)\n",
    "print(t3.shape)  # 查看形状       torch.Size([2, 2, 2])\n",
    "print(t3.dtype)  # 查看数据类型   torch.int64\n",
    "print(t3.device) # 查看存储设备   cpu\n",
    "\n",
    "# 从NumPy数组创建Tensor\n",
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "t_np = torch.tensor(arr)\n",
    "print(t_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bd0e0",
   "metadata": {},
   "source": [
    "PyTorch里的数据类型主要为：\n",
    "\n",
    "1. 整数型 torch.uint8、torch.int32、torch.int64。其中torch.int64为默认的整数类型。\n",
    "\n",
    "2. 浮点型 torch.float16、torch.bfloat16、 torch.float32、torch.float64，其中torch.- float32为默认的浮点数据类型。\n",
    "\n",
    "3. 布尔型 torch.bool\n",
    "\n",
    "torch.float32为全精度，其他为半精度，一般情况下模型训练在全精度下进行。\n",
    "\n",
    "混合精度训练（全精度+半精度），可以加速训练，节省显存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd75b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([False, False,  True,  True,  True])\n",
      "tensor([3, 4, 5])\n",
      "tensor([1, 2, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "#自己指定数据类型\n",
    "t_float = torch.tensor([1,2,3], dtype=torch.float32)\n",
    "print(t_float)  # tensor([1., 2., 3.])\n",
    "\n",
    "#布尔型torch.bool\n",
    "#Bool类型在PyTorch里可以进行高效的索引操作\n",
    "x = torch.tensor([1,2,3,4,5])\n",
    "mask = x > 2  # 生成布尔掩码\n",
    "print(mask)  # tensor([False, False,  True,  True,  True])\n",
    "print(x[mask])  # 使用布尔掩码进行索引：tensor([3, 4, 5])\n",
    "x[mask] = 0\n",
    "print(x)  # tensor([1, 2, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Software\\Anaconda\\envs\\ml_full\\lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 创建tensor指定设备（默认在CPU/内存上）\n",
    "#创建GPU/显存上的Tensor\n",
    "t_gpu = torch.tensor([1,2,3],device = \"cuda\")\n",
    "print(t_gpu.device)   # cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor:tensor([[0.9195, 0.8634, 0.2596],\n",
      "        [0.8207, 0.0605, 0.3544]])\n",
      "randn_tensor:tensor([[ 1.0194, -1.0777,  1.4600],\n",
      "        [-0.0186,  0.2078,  1.8254]])\n",
      "ones_tensor:tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "zeros_tensor:tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "twos_tensor:tensor([[2, 2, 2],\n",
      "        [2, 2, 2]])\n",
      "eye_tensor:tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#指定值或随机值填充tensor、指定tensor形状\n",
    "shape = (2,3) #设定张量的形状，即Tensor的维度\n",
    "rand_tensor = torch.rand(shape) #创建元素值在[0,1)之间均匀分布的tensor\n",
    "print(f\"rand_tensor:{rand_tensor}\")\n",
    "randn_tensor = torch.randn(shape) #创建一个元素值服从标准正态分布的tensor（均值0，标准差1）\n",
    "print(f\"randn_tensor:{randn_tensor}\")\n",
    "ones_tensor = torch.ones(shape) #创建一个所有元素值都为1的tensor\n",
    "print(f\"ones_tensor:{ones_tensor}\")\n",
    "zeros_tensor = torch.zeros(shape) #创建一个所有元素值都为0的tensor\n",
    "print(f\"zeros_tensor:{zeros_tensor}\")\n",
    "\n",
    "#torch.full(size,value) 全value张量\n",
    "twos_tensor = torch.full(shape,2) #创建一个所有元素值都为2的tensor\n",
    "print(f\"twos_tensor:{twos_tensor}\")\n",
    "eye_tensor = torch.eye(3) #创建一个3x3的单位矩阵\n",
    "print(f\"eye_tensor:{eye_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe623cb",
   "metadata": {},
   "source": [
    "### Tensor的属性\n",
    "__torch.randn(size, *, dtype=None, device=None, requires_grad=False)__\n",
    "\n",
    "1. tensor形状（shape/size）\n",
    "\n",
    "2. tensor内元素类型（dtype:默认torch.float32）\n",
    "\n",
    "3. tensor的设备（device:CPU/GPU）\n",
    "\n",
    "4. 计算梯度（requires_grad:是否需要计算梯度，用于深度学习）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor:torch.Size([3, 4])\n",
      "Datatype of tensor:torch.float32\n",
      "Device tensor is stored on:cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor:{tensor.shape}\") # tensor形状\n",
    "print(f\"Datatype of tensor:{tensor.dtype}\") # tensor内元素类型\n",
    "print(f\"Device tensor is stored on:{tensor.device}\") #tensor的设备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b26d8b9",
   "metadata": {},
   "source": [
    "### Tensor的操作\n",
    "1. 形状变换 \n",
    "- x = x.reshape()   按元素顺序重新组织维度\n",
    "- x = x.permute()   改变元素顺序，交换tensor的维度（对于二维矩阵就是行列互换，转置）\n",
    "\n",
    "__在PyTorch里，张量的维度（dimension/axis）是用0，1，2...表示（从0开始！！！）__\n",
    "\n",
    "__第0个维度是行，第1个维度是列__\n",
    "\n",
    "2. 数字运算\n",
    "\n",
    "3. 统计函数\n",
    "\n",
    "4. 索引、切片\n",
    "\n",
    "5. 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15ef0b",
   "metadata": {},
   "source": [
    "#### 1、形状变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ae9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9341,  0.4838, -0.2445, -1.1633],\n",
      "        [-0.9069,  1.6950, -0.4087, -0.6391],\n",
      "        [-0.9152,  1.0841, -0.1041,  0.6078],\n",
      "        [ 0.2125,  0.0835, -1.6805, -0.4007]])\n",
      "tensor([-1.9341,  0.4838, -0.2445, -1.1633, -0.9069,  1.6950, -0.4087, -0.6391,\n",
      "        -0.9152,  1.0841, -0.1041,  0.6078,  0.2125,  0.0835, -1.6805, -0.4007])\n",
      "tensor([[-1.9341, -0.9069, -0.9152,  0.2125],\n",
      "        [ 0.4838,  1.6950,  1.0841,  0.0835],\n",
      "        [-0.2445, -0.4087, -0.1041, -1.6805],\n",
      "        [-1.1633, -0.6391,  0.6078, -0.4007]])\n",
      "torch.Size([1, 4]) tensor([[1, 2, 3, 4]])\n",
      "torch.Size([4, 1]) tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.randn(4,4) \n",
    "x_reshape = x.reshape(16)   #展平为1维向量\n",
    "x_permute = x.permute(1,0)  #交换第0维和第1维，转置\n",
    "print(x)\n",
    "print(x_reshape)\n",
    "print(x_permute)\n",
    "\n",
    "# 对于二维tensor,可用torch.t()转置\n",
    "x_t = torch.t(x)\n",
    "\n",
    "# 扩展tensor的维度，可使用torch.unsqueeze()\n",
    "x = torch.tensor([1,2,3,4])\n",
    "# 扩展第0维\n",
    "x_0 = x.unsqueeze(0) \n",
    "print(x_0.shape,x_0)    # torch.Size([1, 4]) tensor([[1, 2, 3, 4]])\n",
    "#扩展第1维\n",
    "x_1 = x.unsqueeze(1)\n",
    "print(x_1.shape,x_1)    # torch.Size([4, 1]) tensor([[1],[2],[3],[4]])\n",
    "\n",
    "# 缩减tensor的维度，可使用torch.squeeze()\n",
    "# 指定需要缩减的维度索引\n",
    "# 若不指定，则会把所有大小为1的维度都去掉\n",
    "x = torch.ones((1,1,3)) # 3维tensor\n",
    "y = x.squeeze(dim = 0) #缩减第0维\n",
    "print(y.shape)  # torch.Size([1, 3])\n",
    "z = x.squeeze()    #缩减所有为1的维度\n",
    "print(z.shape)  # torch.Size([3]) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e86b4",
   "metadata": {},
   "source": [
    "#### 2、数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b6aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = torch.ones((2,3))\n",
    "b = torch.ones((2,3))\n",
    "#逐元素运算（维度不变）\n",
    "print(a + b) # 加法\n",
    "print(a - b) # 减法\n",
    "print(a * b) # 乘法\n",
    "print(a / b) # 除法\n",
    "#矩阵乘法（改变维度）\n",
    "print(a @ b.T)  #b.T是b的转置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f9047",
   "metadata": {},
   "source": [
    "#### 3、统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.tensor([[1,3],[1,3],[1,3]])\n",
    "# 计算均值：整数tensor不能直接计算均值，需要先转换为浮点型\n",
    "x = x.float()\n",
    "mean_x = x.mean() # 求整个矩阵的均值 \n",
    "x_sum = x.sum()   # 求和\n",
    "x_std = x.std()   # 求标准差\n",
    "x_var = x.var()   # 求方差\n",
    "x_max = x.max()   # 求最大值\n",
    "x_min = x.min()   # 求最小值\n",
    "\n",
    "# 求均值是把指定维度消掉，其他维度保留!!!!!\n",
    "x1_mean = x.mean(dim=0) # 消掉第0维，按列求均值\n",
    "x2_mean = x.mean(dim=1) # 消掉第1维，按行求均值\n",
    "\n",
    "# 统计后保持原来维度不变，不消灭统计的维度\n",
    "# 指定参数 keepdim=True\n",
    "x1_mean_keepdim = x.mean(dim=0,keepdim=True) # 保持第0维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb8844",
   "metadata": {},
   "source": [
    "#### 4、索引与切片\n",
    "\n",
    "计数与C语言风格类似，从0开始计数，左闭右开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor([4, 5, 6])\n",
      "tensor([2, 5, 8])\n",
      "tensor([[2, 3],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "print(x[0,0])  # 访问第1行第1列元素，返回tensor(1)       \n",
    "print(x[1])    # 访问第2行所有元素，返回一个1维tensor\n",
    "print(x[:,1])  # 访问第2列所有元素，返回一个1维tensor\n",
    "print(x[0:2,1:3]) # 访问第1-2行，第2-3列元素，返回一个2维tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501f7bce",
   "metadata": {},
   "source": [
    "#### 5、广播机制\n",
    "\n",
    "原则上来说，tensor的所有的逐元素运算都要求两个tensor的形状必须完全一致。\n",
    "\n",
    "但在实际中，假如我们有一个tensor：t1。t1的shape为（3，2）。我们想给t1的每个元素都加上1。\n",
    "\n",
    "此时我们不必构造一个shape为（3,2），元素全为1的tensor再进行相加。\n",
    "\n",
    "我们可以直接写 t1 +1，PyTorch内部会虚拟扩展出一个形状为（3,2）的tensor，再和t1相加。\n",
    "\n",
    "这种机制，就是广播机制。\n",
    "\n",
    "__PyTorch 在进行广播计算时，并不会真的复制数据，而是通过调整张量的索引方式（Strided Memory Access）来实现逐元素计算。__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4d45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3812, -1.1607],\n",
      "        [ 0.3348,  1.7176],\n",
      "        [-0.4116, -0.4875]])\n",
      "tensor([[ 1.3812, -0.1607],\n",
      "        [ 1.3348,  2.7176],\n",
      "        [ 0.5884,  0.5125]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,2))\n",
    "print(t1)\n",
    "print(t1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa6ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1.])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones((3,2))\n",
    "print(t1)\n",
    "t2 = torch.ones(2)\n",
    "print(t2)\n",
    "print(t1 + t2)  # 广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ceb734",
   "metadata": {},
   "source": [
    "广播机制的一般原则：\n",
    "\n",
    "1. __维度对齐__ ：如果两个tensor的维度数不同，则在较小维度的tensor的形状前面补1，直到两个tensor的维度数相同。\n",
    "\n",
    "2. __扩展维度__ ：在 **维度值为1** 的维度上，通过虚拟复制，让两个tensor的维度值相等。\n",
    "\n",
    "如果两个tensor在某个维度上的长度不匹配，且两个tensor在该维度上的长度都不为1，则报错。\n",
    "\n",
    "3. __进行按位计算__ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e56f67",
   "metadata": {},
   "source": [
    "### 利用GPU加速计算 —— 大幅度加速\n",
    "\n",
    "默认创建的tensor都是在CPU/内存上。让tensor转移到GPU/显存上：\n",
    "\n",
    "1. 创建时，设定tensor的设备为“cuda”\n",
    "\n",
    "2. 将cpu上的tensor通过to(\"cuda\")方法转移到GPU上\n",
    "\n",
    "检查你的环境里是否有可用的英伟达GPU: **torch.cuda.is_available()** \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ad81f",
   "metadata": {},
   "source": [
    "A@B 是 torch.matmul(A,B) 的简写,可处理高维度的tensor(批量矩阵乘法)\n",
    "\n",
    "torch.mm(A,B) 只能用于二维矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afab04c",
   "metadata": {},
   "source": [
    "### Tensor在不同设备上的计算原则\n",
    "模型和张量需要显式移动到目标设备上（如 GPU）；\n",
    "\n",
    "所有参与同一计算的张量必须位于相同设备，计算结果也会保留在该设备上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
