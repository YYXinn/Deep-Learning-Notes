{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e731534",
   "metadata": {},
   "source": [
    "# 初识PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dea870",
   "metadata": {},
   "source": [
    "### 梯度下降法动手实现多元线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "\"\"\"打印torch版本\"\"\"\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 14631.285714285714\n",
      "Iteration 5000: Loss = 54.61083833560779\n",
      "Iteration 10000: Loss = 49.343214206473796\n",
      "Iteration 15000: Loss = 44.647615872447524\n",
      "Iteration 20000: Loss = 40.46192547875196\n",
      "Iteration 25000: Loss = 36.73077071942799\n",
      "Iteration 30000: Loss = 33.404792319545855\n",
      "Iteration 35000: Loss = 30.439991063623914\n",
      "Iteration 40000: Loss = 27.7971457320286\n",
      "Iteration 45000: Loss = 25.4412942452602\n",
      "Iteration 50000: Loss = 23.341271152210684\n",
      "Iteration 55000: Loss = 21.469295343841374\n",
      "Iteration 60000: Loss = 19.80060253816774\n",
      "Iteration 65000: Loss = 18.31311767471115\n",
      "Iteration 70000: Loss = 16.987162884536332\n",
      "Iteration 75000: Loss = 15.80519717262881\n",
      "Iteration 80000: Loss = 14.751584368876536\n",
      "Iteration 85000: Loss = 13.81238627789335\n",
      "Iteration 90000: Loss = 12.975178291273503\n",
      "Iteration 95000: Loss = 12.228885023017044\n",
      "Iteration 100000: Loss = 11.563633793757345\n",
      "Iteration 105000: Loss = 10.970624025539278\n",
      "Iteration 110000: Loss = 10.44201081937723\n",
      "Iteration 115000: Loss = 9.970801175447638\n",
      "Iteration 120000: Loss = 9.550761483014698\n",
      "Iteration 125000: Loss = 9.176335056280791\n",
      "Iteration 130000: Loss = 8.842568625244807\n",
      "Iteration 135000: Loss = 8.54504680912167\n",
      "Iteration 140000: Loss = 8.279833705471395\n",
      "Iteration 145000: Loss = 8.043420822327972\n",
      "Iteration 150000: Loss = 7.8326806645191125\n",
      "Iteration 155000: Loss = 7.644825360177089\n",
      "Iteration 160000: Loss = 7.477369780109347\n",
      "Iteration 165000: Loss = 7.328098662138923\n",
      "Iteration 170000: Loss = 7.195037305502635\n",
      "Iteration 175000: Loss = 7.076425447625239\n",
      "Iteration 180000: Loss = 6.970693977687195\n",
      "Iteration 185000: Loss = 6.876444178928949\n",
      "Iteration 190000: Loss = 6.792429225091967\n",
      "Iteration 195000: Loss = 6.7175376862109415\n",
      "Iteration 200000: Loss = 6.650778825558073\n",
      "Iteration 205000: Loss = 6.591269493232095\n",
      "Iteration 210000: Loss = 6.538222443009152\n",
      "Iteration 215000: Loss = 6.4909359178982795\n",
      "Iteration 220000: Loss = 6.448784366631011\n",
      "Iteration 225000: Loss = 6.411210168271514\n",
      "Iteration 230000: Loss = 6.377716255475671\n",
      "Iteration 235000: Loss = 6.347859538809575\n",
      "Iteration 240000: Loss = 6.3212450451398485\n",
      "Iteration 245000: Loss = 6.297520692552142\n",
      "Iteration 250000: Loss = 6.276372632675885\n",
      "Iteration 255000: Loss = 6.257521098798249\n",
      "Iteration 260000: Loss = 6.2407167048436705\n",
      "Iteration 265000: Loss = 6.225737146256149\n",
      "Iteration 270000: Loss = 6.212384259143302\n",
      "Iteration 275000: Loss = 6.200481398774859\n",
      "Iteration 280000: Loss = 6.189871102757735\n",
      "Iteration 285000: Loss = 6.180413007973129\n",
      "Iteration 290000: Loss = 6.171981993719801\n",
      "Iteration 295000: Loss = 6.164466526497279\n",
      "Iteration 300000: Loss = 6.15776718453491\n",
      "Iteration 305000: Loss = 6.1517953425456895\n",
      "Iteration 310000: Loss = 6.146471999306156\n",
      "Iteration 315000: Loss = 6.141726732553061\n",
      "Iteration 320000: Loss = 6.137496767370641\n",
      "Iteration 325000: Loss = 6.133726145743641\n",
      "Iteration 330000: Loss = 6.13036498629186\n",
      "Iteration 335000: Loss = 6.12736882439122\n",
      "Iteration 340000: Loss = 6.124698023953558\n",
      "Iteration 345000: Loss = 6.122317253082604\n",
      "Iteration 350000: Loss = 6.120195016669854\n",
      "Iteration 355000: Loss = 6.118303239747313\n",
      "Iteration 360000: Loss = 6.116616896084876\n",
      "Iteration 365000: Loss = 6.1151136771197505\n",
      "Iteration 370000: Loss = 6.113773696837319\n",
      "Iteration 375000: Loss = 6.112579228699936\n",
      "Iteration 380000: Loss = 6.111514471143447\n",
      "Iteration 385000: Loss = 6.110565338539156\n",
      "Iteration 390000: Loss = 6.109719274855353\n",
      "Iteration 395000: Loss = 6.1089650875543535\n",
      "Iteration 400000: Loss = 6.108292799527127\n",
      "Iteration 405000: Loss = 6.107693517106209\n",
      "Iteration 410000: Loss = 6.107159312411956\n",
      "Iteration 415000: Loss = 6.106683118474978\n",
      "Iteration 420000: Loss = 6.106258635747652\n",
      "Iteration 425000: Loss = 6.105880248767304\n",
      "Iteration 430000: Loss = 6.105542951870199\n",
      "Iteration 435000: Loss = 6.105242282970915\n",
      "Iteration 440000: Loss = 6.104974264534534\n",
      "Iteration 445000: Loss = 6.1047353509575135\n",
      "Iteration 450000: Loss = 6.104522381663024\n",
      "Iteration 455000: Loss = 6.104332539290108\n",
      "Iteration 460000: Loss = 6.1041633124226635\n",
      "Iteration 465000: Loss = 6.104012462366306\n",
      "Iteration 470000: Loss = 6.1038779935325485\n",
      "Iteration 475000: Loss = 6.103758127039383\n",
      "Iteration 480000: Loss = 6.103651277178471\n",
      "Iteration 485000: Loss = 6.1035560304378365\n",
      "Iteration 490000: Loss = 6.103471126802769\n",
      "Iteration 495000: Loss = 6.103395443087048\n",
      "Iteration 500000: Loss = 6.103327978074175\n",
      "Iteration 505000: Loss = 6.103267839272659\n",
      "Iteration 510000: Loss = 6.103214231109098\n",
      "Iteration 515000: Loss = 6.103166444403439\n",
      "Iteration 520000: Loss = 6.103123846987592\n",
      "Iteration 525000: Loss = 6.103085875342335\n",
      "Iteration 530000: Loss = 6.103052027142463\n",
      "Iteration 535000: Loss = 6.103021854611737\n",
      "Iteration 540000: Loss = 6.10299495859917\n",
      "Iteration 545000: Loss = 6.102970983298552\n",
      "Iteration 550000: Loss = 6.102949611541787\n",
      "Iteration 555000: Loss = 6.102930560602894\n",
      "Iteration 560000: Loss = 6.102913578457846\n",
      "Iteration 565000: Loss = 6.102898440450646\n",
      "Iteration 570000: Loss = 6.102884946321327\n",
      "Iteration 575000: Loss = 6.1028729175565894\n",
      "Iteration 580000: Loss = 6.102862195028528\n",
      "Iteration 585000: Loss = 6.102852636889274\n",
      "Iteration 590000: Loss = 6.1028441166946426\n",
      "Iteration 595000: Loss = 6.102836521731413\n",
      "Iteration 600000: Loss = 6.102829751526116\n",
      "Iteration 605000: Loss = 6.102823716516079\n",
      "Iteration 610000: Loss = 6.102818336864345\n",
      "Iteration 615000: Loss = 6.102813541403814\n",
      "Iteration 620000: Loss = 6.102809266695496\n",
      "Iteration 625000: Loss = 6.102805456189539\n",
      "Iteration 630000: Loss = 6.102802059476896\n",
      "Iteration 635000: Loss = 6.102799031622629\n",
      "Iteration 640000: Loss = 6.102796332571358\n",
      "Iteration 645000: Loss = 6.102793926617549\n",
      "Iteration 650000: Loss = 6.1027917819328446\n",
      "Iteration 655000: Loss = 6.102789870145338\n",
      "Iteration 660000: Loss = 6.102788165964126\n",
      "Iteration 665000: Loss = 6.102786646844625\n",
      "Iteration 670000: Loss = 6.102785292690475\n",
      "Iteration 675000: Loss = 6.102784085587703\n",
      "Iteration 680000: Loss = 6.102783009567482\n",
      "Iteration 685000: Loss = 6.102782050395318\n",
      "Iteration 690000: Loss = 6.1027811953822475\n",
      "Iteration 695000: Loss = 6.102780433217392\n",
      "Iteration 700000: Loss = 6.102779753818157\n",
      "Iteration 705000: Loss = 6.102779148196737\n",
      "Iteration 710000: Loss = 6.102778608341391\n",
      "Iteration 715000: Loss = 6.10277812711045\n",
      "Iteration 720000: Loss = 6.102777698137712\n",
      "Iteration 725000: Loss = 6.102777315748272\n",
      "Iteration 730000: Loss = 6.102776974883581\n",
      "Iteration 735000: Loss = 6.102776671034286\n",
      "Iteration 740000: Loss = 6.102776400180901\n",
      "Iteration 745000: Loss = 6.10277615874012\n",
      "Iteration 750000: Loss = 6.102775943518139\n",
      "Iteration 755000: Loss = 6.102775751667664\n",
      "Iteration 760000: Loss = 6.10277558065075\n",
      "Iteration 765000: Loss = 6.102775428205072\n",
      "Iteration 770000: Loss = 6.102775292313884\n",
      "Iteration 775000: Loss = 6.102775171179447\n",
      "Iteration 780000: Loss = 6.102775063199412\n",
      "Iteration 785000: Loss = 6.102774966945155\n",
      "Iteration 790000: Loss = 6.102774881143448\n",
      "Iteration 795000: Loss = 6.102774804659176\n",
      "Iteration 800000: Loss = 6.102774736480508\n",
      "Iteration 805000: Loss = 6.10277467570562\n",
      "Iteration 810000: Loss = 6.102774621530389\n",
      "Iteration 815000: Loss = 6.102774573238235\n",
      "Iteration 820000: Loss = 6.1027745301902225\n",
      "Iteration 825000: Loss = 6.102774491816951\n",
      "Iteration 830000: Loss = 6.102774457610677\n",
      "Iteration 835000: Loss = 6.102774427119003\n",
      "Iteration 840000: Loss = 6.102774399938503\n",
      "Iteration 845000: Loss = 6.102774375709631\n",
      "Iteration 850000: Loss = 6.102774354111811\n",
      "Iteration 855000: Loss = 6.102774334859297\n",
      "Iteration 860000: Loss = 6.102774317697544\n",
      "Iteration 865000: Loss = 6.102774302399388\n",
      "Iteration 870000: Loss = 6.102774288762548\n",
      "Iteration 875000: Loss = 6.102774276606523\n",
      "Iteration 880000: Loss = 6.102774265770586\n",
      "Iteration 885000: Loss = 6.102774256111331\n",
      "Iteration 890000: Loss = 6.102774247501037\n",
      "Iteration 895000: Loss = 6.102774239825722\n",
      "Iteration 900000: Loss = 6.10277423298389\n",
      "Iteration 905000: Loss = 6.102774226885081\n",
      "Iteration 910000: Loss = 6.102774221448549\n",
      "Iteration 915000: Loss = 6.102774216602333\n",
      "Iteration 920000: Loss = 6.102774212282418\n",
      "Iteration 925000: Loss = 6.102774208431567\n",
      "Iteration 930000: Loss = 6.102774204998958\n",
      "Iteration 935000: Loss = 6.102774201939066\n",
      "Iteration 940000: Loss = 6.10277419921145\n",
      "Iteration 945000: Loss = 6.102774196780061\n",
      "Iteration 950000: Loss = 6.102774194612702\n",
      "Iteration 955000: Loss = 6.102774192680682\n",
      "Iteration 960000: Loss = 6.10277419095853\n",
      "Iteration 965000: Loss = 6.102774189423289\n",
      "Iteration 970000: Loss = 6.102774188054838\n",
      "Iteration 975000: Loss = 6.102774186834966\n",
      "Iteration 980000: Loss = 6.102774185747541\n",
      "Iteration 985000: Loss = 6.1027741847782435\n",
      "Iteration 990000: Loss = 6.1027741839141685\n",
      "Iteration 995000: Loss = 6.102774183143903\n",
      "Iteration 1000000: Loss = 6.102774182457317\n",
      "Iteration 1005000: Loss = 6.102774181845312\n",
      "Iteration 1010000: Loss = 6.102774181299758\n",
      "Iteration 1015000: Loss = 6.102774180813398\n",
      "Iteration 1020000: Loss = 6.102774180379901\n",
      "Iteration 1025000: Loss = 6.1027741799934825\n",
      "Iteration 1030000: Loss = 6.102774179649027\n",
      "Iteration 1035000: Loss = 6.102774179341987\n",
      "Iteration 1040000: Loss = 6.102774179068224\n",
      "Iteration 1045000: Loss = 6.1027741788242125\n",
      "Iteration 1050000: Loss = 6.102774178606731\n",
      "Iteration 1055000: Loss = 6.1027741784128455\n",
      "Iteration 1060000: Loss = 6.102774178240044\n",
      "Iteration 1065000: Loss = 6.10277417808603\n",
      "Iteration 1070000: Loss = 6.102774177948649\n",
      "Iteration 1075000: Loss = 6.102774177826256\n",
      "Iteration 1080000: Loss = 6.102774177717142\n",
      "Iteration 1085000: Loss = 6.1027741776198\n",
      "Iteration 1090000: Loss = 6.102774177533165\n",
      "Iteration 1095000: Loss = 6.102774177455832\n",
      "Iteration 1100000: Loss = 6.102774177386906\n",
      "Iteration 1105000: Loss = 6.102774177325512\n",
      "Iteration 1110000: Loss = 6.102774177270767\n",
      "Iteration 1115000: Loss = 6.102774177222001\n",
      "Iteration 1120000: Loss = 6.1027741771784685\n",
      "Iteration 1125000: Loss = 6.102774177139745\n",
      "Iteration 1130000: Loss = 6.102774177105092\n",
      "Iteration 1135000: Loss = 6.102774177074333\n",
      "Iteration 1140000: Loss = 6.102774177046798\n",
      "Iteration 1145000: Loss = 6.102774177022328\n",
      "Iteration 1150000: Loss = 6.102774177000518\n",
      "Iteration 1155000: Loss = 6.102774176981032\n",
      "Iteration 1160000: Loss = 6.10277417696374\n",
      "Iteration 1165000: Loss = 6.102774176948267\n",
      "Iteration 1170000: Loss = 6.102774176934468\n",
      "Iteration 1175000: Loss = 6.102774176922217\n",
      "Iteration 1180000: Loss = 6.102774176911213\n",
      "Iteration 1185000: Loss = 6.102774176901476\n",
      "Iteration 1190000: Loss = 6.102774176892796\n",
      "Iteration 1195000: Loss = 6.102774176885043\n",
      "Iteration 1200000: Loss = 6.102774176878093\n",
      "Iteration 1205000: Loss = 6.102774176871977\n",
      "Iteration 1210000: Loss = 6.102774176866461\n",
      "Iteration 1215000: Loss = 6.102774176861528\n",
      "Iteration 1220000: Loss = 6.1027741768571975\n",
      "Iteration 1225000: Loss = 6.102774176853338\n",
      "Iteration 1230000: Loss = 6.102774176849876\n",
      "Iteration 1235000: Loss = 6.10277417684675\n",
      "Iteration 1240000: Loss = 6.102774176843982\n",
      "Iteration 1245000: Loss = 6.10277417684152\n",
      "Iteration 1250000: Loss = 6.102774176839309\n",
      "Iteration 1255000: Loss = 6.102774176837388\n",
      "Iteration 1260000: Loss = 6.102774176835651\n",
      "Iteration 1265000: Loss = 6.102774176834124\n",
      "Iteration 1270000: Loss = 6.102774176832709\n",
      "Iteration 1275000: Loss = 6.102774176831466\n",
      "Iteration 1280000: Loss = 6.102774176830406\n",
      "Iteration 1285000: Loss = 6.102774176829414\n",
      "Iteration 1290000: Loss = 6.102774176828526\n",
      "Iteration 1295000: Loss = 6.102774176827746\n",
      "Iteration 1300000: Loss = 6.102774176827089\n",
      "Iteration 1305000: Loss = 6.102774176826429\n",
      "Iteration 1310000: Loss = 6.102774176825903\n",
      "Iteration 1315000: Loss = 6.102774176825378\n",
      "Iteration 1320000: Loss = 6.102774176824947\n",
      "Iteration 1325000: Loss = 6.102774176824596\n",
      "Iteration 1330000: Loss = 6.102774176824227\n",
      "Iteration 1335000: Loss = 6.102774176823905\n",
      "Iteration 1340000: Loss = 6.10277417682366\n",
      "Iteration 1345000: Loss = 6.102774176823384\n",
      "Iteration 1350000: Loss = 6.102774176823174\n",
      "Iteration 1355000: Loss = 6.102774176822942\n",
      "Iteration 1360000: Loss = 6.102774176822818\n",
      "Iteration 1365000: Loss = 6.102774176822615\n",
      "Iteration 1370000: Loss = 6.102774176822495\n",
      "Iteration 1375000: Loss = 6.102774176822377\n",
      "Iteration 1380000: Loss = 6.10277417682227\n",
      "Iteration 1385000: Loss = 6.1027741768221615\n",
      "Iteration 1390000: Loss = 6.102774176822111\n",
      "Iteration 1395000: Loss = 6.102774176822004\n",
      "Iteration 1400000: Loss = 6.1027741768219546\n",
      "Iteration 1405000: Loss = 6.102774176821862\n",
      "Iteration 1410000: Loss = 6.102774176821839\n",
      "Iteration 1415000: Loss = 6.102774176821792\n",
      "Iteration 1420000: Loss = 6.102774176821754\n",
      "Iteration 1425000: Loss = 6.1027741768216766\n",
      "Iteration 1430000: Loss = 6.10277417682166\n",
      "Iteration 1435000: Loss = 6.102774176821648\n",
      "Iteration 1440000: Loss = 6.102774176821599\n",
      "Iteration 1445000: Loss = 6.102774176821555\n",
      "Iteration 1450000: Loss = 6.102774176821568\n",
      "Iteration 1455000: Loss = 6.102774176821507\n",
      "Iteration 1460000: Loss = 6.102774176821491\n",
      "Iteration 1465000: Loss = 6.102774176821514\n",
      "Iteration 1470000: Loss = 6.102774176821486\n",
      "Iteration 1475000: Loss = 6.102774176821448\n",
      "Iteration 1480000: Loss = 6.102774176821442\n",
      "Iteration 1485000: Loss = 6.102774176821463\n",
      "Iteration 1490000: Loss = 6.1027741768214225\n",
      "Iteration 1495000: Loss = 6.102774176821433\n",
      "Iteration 1500000: Loss = 6.102774176821437\n",
      "Iteration 1505000: Loss = 6.102774176821397\n",
      "Iteration 1510000: Loss = 6.102774176821398\n",
      "Iteration 1515000: Loss = 6.102774176821389\n",
      "Iteration 1520000: Loss = 6.102774176821387\n",
      "Iteration 1525000: Loss = 6.102774176821359\n",
      "Iteration 1530000: Loss = 6.10277417682143\n",
      "Iteration 1535000: Loss = 6.102774176821389\n",
      "Iteration 1540000: Loss = 6.102774176821419\n",
      "Iteration 1545000: Loss = 6.102774176821358\n",
      "Iteration 1550000: Loss = 6.102774176821386\n",
      "Iteration 1555000: Loss = 6.102774176821406\n",
      "Iteration 1560000: Loss = 6.102774176821382\n",
      "Iteration 1565000: Loss = 6.1027741768213835\n",
      "Iteration 1570000: Loss = 6.10277417682137\n",
      "Iteration 1575000: Loss = 6.10277417682136\n",
      "Iteration 1580000: Loss = 6.102774176821389\n",
      "Iteration 1585000: Loss = 6.102774176821363\n",
      "Iteration 1590000: Loss = 6.102774176821414\n",
      "Iteration 1595000: Loss = 6.102774176821372\n",
      "Iteration 1600000: Loss = 6.102774176821356\n",
      "Iteration 1605000: Loss = 6.102774176821383\n",
      "Iteration 1610000: Loss = 6.102774176821358\n",
      "Iteration 1615000: Loss = 6.102774176821356\n",
      "Iteration 1620000: Loss = 6.10277417682138\n",
      "Iteration 1625000: Loss = 6.1027741768213515\n",
      "Iteration 1630000: Loss = 6.102774176821362\n",
      "Iteration 1635000: Loss = 6.102774176821378\n",
      "Iteration 1640000: Loss = 6.102774176821372\n",
      "Iteration 1645000: Loss = 6.1027741768213515\n",
      "Iteration 1650000: Loss = 6.102774176821398\n",
      "Iteration 1655000: Loss = 6.102774176821346\n",
      "Iteration 1660000: Loss = 6.102774176821375\n",
      "Iteration 1665000: Loss = 6.102774176821399\n",
      "Iteration 1670000: Loss = 6.102774176821357\n",
      "Iteration 1675000: Loss = 6.102774176821351\n",
      "Iteration 1680000: Loss = 6.102774176821351\n",
      "Iteration 1685000: Loss = 6.10277417682137\n",
      "Iteration 1690000: Loss = 6.102774176821344\n",
      "Iteration 1695000: Loss = 6.102774176821415\n",
      "Iteration 1700000: Loss = 6.102774176821372\n",
      "Iteration 1705000: Loss = 6.102774176821347\n",
      "Iteration 1710000: Loss = 6.1027741768213755\n",
      "Iteration 1715000: Loss = 6.102774176821351\n",
      "Iteration 1720000: Loss = 6.102774176821379\n",
      "Iteration 1725000: Loss = 6.102774176821342\n",
      "Iteration 1730000: Loss = 6.1027741768213515\n",
      "Iteration 1735000: Loss = 6.10277417682138\n",
      "Iteration 1740000: Loss = 6.1027741768213435\n",
      "Iteration 1745000: Loss = 6.102774176821386\n",
      "Iteration 1750000: Loss = 6.102774176821369\n",
      "Iteration 1755000: Loss = 6.102774176821368\n",
      "Iteration 1760000: Loss = 6.1027741768213835\n",
      "Iteration 1765000: Loss = 6.102774176821383\n",
      "Iteration 1770000: Loss = 6.102774176821358\n",
      "Iteration 1775000: Loss = 6.10277417682134\n",
      "Iteration 1780000: Loss = 6.102774176821337\n",
      "Iteration 1785000: Loss = 6.102774176821379\n",
      "Iteration 1790000: Loss = 6.1027741768214\n",
      "Iteration 1795000: Loss = 6.102774176821379\n",
      "Iteration 1800000: Loss = 6.102774176821362\n",
      "Iteration 1805000: Loss = 6.10277417682132\n",
      "Iteration 1810000: Loss = 6.10277417682134\n",
      "Iteration 1815000: Loss = 6.102774176821357\n",
      "Iteration 1820000: Loss = 6.102774176821383\n",
      "Iteration 1825000: Loss = 6.10277417682136\n",
      "Iteration 1830000: Loss = 6.102774176821357\n",
      "Iteration 1835000: Loss = 6.102774176821346\n",
      "Iteration 1840000: Loss = 6.102774176821364\n",
      "Iteration 1845000: Loss = 6.102774176821312\n",
      "Iteration 1850000: Loss = 6.1027741768213435\n",
      "Iteration 1855000: Loss = 6.102774176821378\n",
      "Iteration 1860000: Loss = 6.102774176821407\n",
      "Iteration 1865000: Loss = 6.102774176821379\n",
      "Iteration 1870000: Loss = 6.102774176821398\n",
      "Iteration 1875000: Loss = 6.102774176821354\n",
      "Iteration 1880000: Loss = 6.1027741768213435\n",
      "Iteration 1885000: Loss = 6.102774176821379\n",
      "Iteration 1890000: Loss = 6.1027741768213515\n",
      "Iteration 1895000: Loss = 6.102774176821379\n",
      "Iteration 1900000: Loss = 6.102774176821339\n",
      "Iteration 1905000: Loss = 6.102774176821348\n",
      "Iteration 1910000: Loss = 6.10277417682138\n",
      "Iteration 1915000: Loss = 6.102774176821411\n",
      "Iteration 1920000: Loss = 6.1027741768214145\n",
      "Iteration 1925000: Loss = 6.1027741768213515\n",
      "Iteration 1930000: Loss = 6.102774176821343\n",
      "Iteration 1935000: Loss = 6.102774176821392\n",
      "Iteration 1940000: Loss = 6.102774176821346\n",
      "Iteration 1945000: Loss = 6.102774176821363\n",
      "Iteration 1950000: Loss = 6.1027741768213355\n",
      "Iteration 1955000: Loss = 6.102774176821335\n",
      "Iteration 1960000: Loss = 6.102774176821358\n",
      "Iteration 1965000: Loss = 6.102774176821327\n",
      "Iteration 1970000: Loss = 6.102774176821334\n",
      "Iteration 1975000: Loss = 6.10277417682137\n",
      "Iteration 1980000: Loss = 6.102774176821372\n",
      "Iteration 1985000: Loss = 6.102774176821353\n",
      "Iteration 1990000: Loss = 6.102774176821337\n",
      "Iteration 1995000: Loss = 6.102774176821372\n",
      "Final parameters: w0 = 94.66279490859776, w1 = 3.0609800363839135, w2 = -22.978584389229724\n"
     ]
    }
   ],
   "source": [
    "# Feature数据\n",
    "X = [[10,3],[20,3],[25,3],[28,2.5],[30,2],[35,2.5],[40,2.5]]\n",
    "y = [60,85,100,120,140,145,163]  # 标签数据\n",
    "# 初始化参数\n",
    "w = [0.0,0.0,0.0]    # w0,w1,w2\n",
    "lr = 0.001           # 学习率\n",
    "iteration = 2000000  # 迭代次数\n",
    "# 梯度下降\n",
    "for it in range(iteration):\n",
    "    # 计算预测值（逐样本）【前向传播】\n",
    "    y_pred = []\n",
    "    for i in range(len(X)):\n",
    "        y_pred.append(w[0] + w[1]*X[i][0] + w[2]*X[i][1])\n",
    "    # 计算损失函数（均方误差MSE）\n",
    "    loss = 0.0\n",
    "    for i in range(len(y)):\n",
    "        loss += (y[i] - y_pred[i])**2\n",
    "    loss /= len(y)\n",
    "    # 手动计算梯度【反向传播】\n",
    "    dw0 = dw1 = dw2 = 0.0\n",
    "    for i in range(len(y)):\n",
    "        dw0 += -2*(y[i]-y_pred[i])\n",
    "        dw1 += -2*(y[i]-y_pred[i])*X[i][0]\n",
    "        dw2 += -2*(y[i]-y_pred[i])*X[i][1]\n",
    "    dw0 /= len(y)\n",
    "    dw1 /= len(y)\n",
    "    dw2 /= len(y)\n",
    "    # 更新参数\n",
    "    w[0] -= lr*dw0\n",
    "    w[1] -= lr*dw1\n",
    "    w[2] -= lr*dw2\n",
    "    # 每5000次迭代输出一次损失函数\n",
    "    if it % 5000 == 0:\n",
    "        print(f\"Iteration {it}: Loss = {loss}\")\n",
    "# 输出最终参数\n",
    "print(f\"Final parameters: w0 = {w[0]}, w1 = {w[1]}, w2 = {w[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
