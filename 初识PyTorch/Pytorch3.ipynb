{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77e2bbe2",
   "metadata": {},
   "source": [
    "## 计算图与自动求梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00864bf",
   "metadata": {},
   "source": [
    "梯度计算：\n",
    "\n",
    "1. 分析自变量x和因变量y的关系\n",
    "\n",
    "2. 形成**计算图**\n",
    "\n",
    "3. 根据链式法则和求导公式计算偏导数，得到梯度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09645b2",
   "metadata": {},
   "source": [
    "### PyTorch自动求梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8571)\n",
      "tensor(1.1429)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1.0,requires_grad=True) # 注意要写1.0，梯度计算必须是浮点数\n",
    "y = torch.tensor(1.0,requires_grad=True)\n",
    "v = 3*x+4*y\n",
    "u = torch.square(v)  # v**2,逐元素平方\n",
    "z = torch.log(u)\n",
    "\n",
    "z.backward()  # 反向传播（backpropagation)，计算梯度\n",
    "print(x.grad)  # dz/dx\n",
    "print(y.grad)  # dz/dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8118ba",
   "metadata": {},
   "source": [
    "#### 为什么需要反向传播计算梯度？\n",
    "深度学习有：输入、模型参数、输出、损失函数\n",
    "\n",
    "训练目标是通过不断更新参数，让loss最小\n",
    "\n",
    "---> 更新参数（eg:SGD算法）让loss最小，需要知道loss对每个参数的导数\n",
    "\n",
    "反向传播就是：从损失函数往回走，利用链式法则自动计算每个参数的梯度，更新模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2d076",
   "metadata": {},
   "source": [
    "#### SGD算法（随机梯度下降，Stochastic Gradient Descent）\n",
    "y = wx + b   (w:weight 权重    b:bias 偏差)\n",
    "\n",
    "参数更新过程：θ被梯度往下推向最小值点\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
